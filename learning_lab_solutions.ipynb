{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denuka1993/DataScienceExercise/blob/main/learning_lab_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a96707e",
      "metadata": {
        "id": "4a96707e"
      },
      "source": [
        "# Learning Lab: Solutions & Explanations\n",
        "\n",
        "**Course:** GIK2Q3 Applied Big Data and Cloud Computing  \n",
        "**Duration:** ~3 hours  \n",
        "**Week:** 4\n",
        "\n",
        "---\n",
        "\n",
        "## üìö About This Notebook\n",
        "\n",
        "This is the **solutions notebook** for the Learning Lab. It contains:\n",
        "\n",
        "- ‚úÖ **Worked solutions** for all exercises\n",
        "- üìñ **Detailed explanations** of key concepts\n",
        "- üí° **Common pitfalls** and how to avoid them\n",
        "- üéØ **Discussion answers** for group activities\n",
        "\n",
        "Use this as a reference for self-study after the lab session."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3967e7ce",
      "metadata": {
        "id": "3967e7ce"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 1: Environment Setup\n",
        "\n",
        "### üìñ Troubleshooting Tips\n",
        "\n",
        "**Common Issues:**\n",
        "- **Java not found:** Most common on Windows. Solution: Use Google Colab as a backup.\n",
        "- **PySpark import fails:** Usually means Java isn't configured. Check JAVA_HOME.\n",
        "- **Memory errors:** Reduce driver memory or use Colab (12GB available).\n",
        "\n",
        "**Tip:** If you're having trouble with local setup, Google Colab is a reliable alternative that works out of the box.\n",
        "\n",
        "### üêç Note: BrokenPipeError Messages\n",
        "\n",
        "You may see `BrokenPipeError: [Errno 32] Broken pipe` messages after some cells. **These are harmless** ‚Äî your code worked fine! Just ignore the traceback if you see the expected output before the error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130e57bd",
      "metadata": {
        "id": "130e57bd"
      },
      "outputs": [],
      "source": [
        "# Environment setup (same as main notebook)\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "try:\n",
        "    import pyspark\n",
        "    print(f\"‚úì PySpark already installed: version {pyspark.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing PySpark...\")\n",
        "    %pip install pyspark -q\n",
        "    import pyspark\n",
        "    print(f\"‚úì PySpark installed: version {pyspark.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f6f9f2",
      "metadata": {
        "id": "d1f6f9f2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Java setup for local installations\n",
        "java_paths = [\n",
        "    \"/opt/homebrew/opt/openjdk@17\",\n",
        "    \"/usr/local/opt/openjdk@17\",\n",
        "    \"/usr/lib/jvm/java-17-openjdk-amd64\",\n",
        "]\n",
        "\n",
        "java_home = os.environ.get(\"JAVA_HOME\")\n",
        "if not java_home:\n",
        "    for path in java_paths:\n",
        "        if os.path.exists(path):\n",
        "            os.environ[\"JAVA_HOME\"] = path\n",
        "            java_home = path\n",
        "            break\n",
        "\n",
        "if java_home:\n",
        "    print(f\"‚úì JAVA_HOME set to: {java_home}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Java not found locally. Colab users can ignore this.\")\n",
        "\n",
        "print(f\"‚úì Pandas version: {pd.__version__}\")\n",
        "print(f\"‚úì NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "617a0d85",
      "metadata": {
        "id": "617a0d85"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: Understanding MapReduce\n",
        "\n",
        "### üìñ Why MapReduce Matters\n",
        "\n",
        "MapReduce is the mental model that underlies all distributed data processing. Even though Spark abstracts it away, understanding Map ‚Üí Shuffle ‚Üí Reduce helps you:\n",
        "\n",
        "1. **Reason about parallelism:** What can run in parallel? What requires coordination?\n",
        "2. **Understand performance:** Why is shuffle expensive? Why does data skew hurt?\n",
        "3. **Debug Spark jobs:** The Spark UI shows stages that map directly to this model\n",
        "\n",
        "**Key Insight:**\n",
        "- Map operations are \"embarrassingly parallel\" ‚Äî each document is independent\n",
        "- Shuffle is the expensive part ‚Äî data must move across the network\n",
        "- Reduce can also be parallel if the operation is associative/commutative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d61ad44",
      "metadata": {
        "id": "6d61ad44"
      },
      "outputs": [],
      "source": [
        "# Sample text data for MapReduce examples\n",
        "text_data = \"\"\"\n",
        "Big data is transforming how we understand the world\n",
        "Data science and machine learning rely on big data\n",
        "The world of data is growing every day\n",
        "Machine learning models need data to learn\n",
        "Big models require big data and big compute\n",
        "\"\"\"\n",
        "\n",
        "lines = text_data.strip().split('\\n')\n",
        "print(f\"We have {len(lines)} 'documents' to process\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd284f60",
      "metadata": {
        "id": "bd284f60"
      },
      "outputs": [],
      "source": [
        "# Standard MapReduce implementation (for reference)\n",
        "def map_function(document):\n",
        "    words = document.lower().split()\n",
        "    return [(word, 1) for word in words]\n",
        "\n",
        "def shuffle(mapped_pairs):\n",
        "    grouped = defaultdict(list)\n",
        "    for key, value in mapped_pairs:\n",
        "        grouped[key].append(value)\n",
        "    return grouped\n",
        "\n",
        "def reduce_function(key, values):\n",
        "    return (key, sum(values))\n",
        "\n",
        "# Execute MapReduce\n",
        "mapped_results = []\n",
        "for doc in lines:\n",
        "    mapped_results.extend(map_function(doc))\n",
        "\n",
        "shuffled = shuffle(mapped_results)\n",
        "final_counts = {k: sum(v) for k, v in shuffled.items()}\n",
        "\n",
        "print(\"Word counts:\")\n",
        "for word, count in sorted(final_counts.items(), key=lambda x: -x[1])[:8]:\n",
        "    print(f\"  {word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e9978a",
      "metadata": {
        "id": "a6e9978a"
      },
      "source": [
        "---\n",
        "\n",
        "## üèãÔ∏è Exercise A: MapReduce Challenge ‚Äî SOLUTIONS\n",
        "\n",
        "### Challenge 1: Filter Words > 3 Characters\n",
        "\n",
        "**Key Concept:** Filtering during the map phase is more efficient than filtering after ‚Äî it means less data to shuffle across the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9ea90d",
      "metadata": {
        "id": "6e9ea90d"
      },
      "outputs": [],
      "source": [
        "# SOLUTION: Challenge 1 - Count only words with more than 3 characters\n",
        "\n",
        "def map_function_filtered(document):\n",
        "    \"\"\"MAP: Emit (word, 1) only for words with more than 3 characters.\"\"\"\n",
        "    words = document.lower().split()\n",
        "    # KEY INSIGHT: Filter during map = less data to shuffle!\n",
        "    return [(word, 1) for word in words if len(word) > 3]\n",
        "\n",
        "# Test the solution\n",
        "mapped_filtered = []\n",
        "for doc in lines:\n",
        "    mapped_filtered.extend(map_function_filtered(doc))\n",
        "\n",
        "shuffled_filtered = shuffle(mapped_filtered)\n",
        "filtered_counts = {k: sum(v) for k, v in shuffled_filtered.items()}\n",
        "\n",
        "print(\"‚úÖ SOLUTION: Words with >3 characters:\")\n",
        "print(\"-\" * 40)\n",
        "for word, count in sorted(filtered_counts.items(), key=lambda x: -x[1])[:8]:\n",
        "    print(f\"  {word}: {count}\")\n",
        "\n",
        "print(f\"\\nüìä We filtered out: {len(final_counts) - len(filtered_counts)} short words\")\n",
        "print(f\"   Original word count: {len(final_counts)}\")\n",
        "print(f\"   After filter: {len(filtered_counts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e59fdf5f",
      "metadata": {
        "id": "e59fdf5f"
      },
      "source": [
        "### üí° Common Mistakes - Challenge 1\n",
        "\n",
        "| Mistake | Why It's Wrong | Correct Approach |\n",
        "|---------|----------------|------------------|\n",
        "| `len(word) >= 3` | Includes 3-letter words | Use `len(word) > 3` |\n",
        "| Filtering after reduce | Works but inefficient | Filter in map phase |\n",
        "| Forgetting `.lower()` | \"Big\" and \"big\" counted separately | Normalize case first |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd08c2a9",
      "metadata": {
        "id": "fd08c2a9"
      },
      "source": [
        "### Challenge 2: Count Capital Words\n",
        "\n",
        "**Key Concept:** The choice of what to emit in map depends on what you're trying to count. You can't always lowercase ‚Äî context matters for your analysis goals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b88bbe",
      "metadata": {
        "id": "39b88bbe"
      },
      "outputs": [],
      "source": [
        "# SOLUTION: Challenge 2 - Count words starting with capital letters\n",
        "\n",
        "original_text = \"\"\"\n",
        "Big data is transforming how we understand the world\n",
        "Data science and machine learning rely on big data\n",
        "The world of data is growing every day\n",
        "Machine learning models need data to learn\n",
        "Big models require big data and big compute\n",
        "\"\"\"\n",
        "\n",
        "def map_capitals(document):\n",
        "    \"\"\"MAP: Emit (word, 1) only for words starting with a capital letter.\"\"\"\n",
        "    words = document.split()\n",
        "    # KEY INSIGHT: Check first character, but keep original case for the key\n",
        "    return [(word, 1) for word in words if word and word[0].isupper()]\n",
        "\n",
        "# Test the solution\n",
        "original_lines = original_text.strip().split('\\n')\n",
        "mapped_capitals = []\n",
        "for doc in original_lines:\n",
        "    mapped_capitals.extend(map_capitals(doc))\n",
        "\n",
        "shuffled_capitals = shuffle(mapped_capitals)\n",
        "capital_counts = {k: sum(v) for k, v in shuffled_capitals.items()}\n",
        "\n",
        "print(\"‚úÖ SOLUTION: Capital words:\")\n",
        "print(\"-\" * 40)\n",
        "for word, count in sorted(capital_counts.items(), key=lambda x: -x[1])[:8]:\n",
        "    print(f\"  {word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a555f1aa",
      "metadata": {
        "id": "a555f1aa"
      },
      "source": [
        "### üí° Discussion Point - Challenge 2\n",
        "\n",
        "**Question:** \"Big\" appears twice but \"big\" (lowercase) appears more times. Should they be combined?\n",
        "\n",
        "**Answer:** It depends on your use case!\n",
        "- For finding proper nouns ‚Üí keep them separate\n",
        "- For general word frequency ‚Üí combine them with `.lower()`\n",
        "\n",
        "This is a great example of how data processing decisions depend on business requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be2f724",
      "metadata": {
        "id": "5be2f724"
      },
      "source": [
        "### Challenge 3: Find the Longest Word\n",
        "\n",
        "**Key Concept:** This is a great example of thinking about what's parallelizable. Finding a maximum is associative ‚Äî you can find the max of maxes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3e2a87",
      "metadata": {
        "id": "9f3e2a87"
      },
      "outputs": [],
      "source": [
        "# SOLUTION: Challenge 3 - Find the longest word using MapReduce pattern\n",
        "\n",
        "def map_longest(document):\n",
        "    \"\"\"MAP: Find the longest word in this document.\"\"\"\n",
        "    words = document.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    # Emit (dummy_key, longest_word_in_this_doc)\n",
        "    # We use a single key so all words go to one reducer\n",
        "    longest = max(words, key=len)\n",
        "    return [(\"longest\", longest)]\n",
        "\n",
        "def reduce_longest(words):\n",
        "    \"\"\"REDUCE: Compare words from all mappers, keep the longest.\"\"\"\n",
        "    return max(words, key=len)\n",
        "\n",
        "# Execute\n",
        "mapped_longest = []\n",
        "for doc in lines:\n",
        "    result = map_longest(doc)\n",
        "    if result:\n",
        "        mapped_longest.append(result[0][1])  # Extract just the word\n",
        "\n",
        "print(\"Words from each document (map phase):\")\n",
        "for i, word in enumerate(mapped_longest):\n",
        "    print(f\"  Doc {i}: '{word}' ({len(word)} chars)\")\n",
        "\n",
        "final_longest = reduce_longest(mapped_longest)\n",
        "print(f\"\\n‚úÖ SOLUTION: Longest word is '{final_longest}' ({len(final_longest)} characters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc71653",
      "metadata": {
        "id": "dcc71653"
      },
      "source": [
        "### üìñ Why This Works in Parallel\n",
        "\n",
        "Finding a maximum is an **associative operation**:\n",
        "```\n",
        "max(max(a, b), max(c, d)) = max(a, b, c, d)\n",
        "```\n",
        "\n",
        "This means we can:\n",
        "1. **Map phase:** Find the longest word in each partition independently\n",
        "2. **Reduce phase:** Compare the \"local maximums\" to find the global maximum\n",
        "\n",
        "**Not all operations parallelize this well!** For example, median requires seeing all data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fd34e75",
      "metadata": {
        "id": "2fd34e75"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 3: SparkSession\n",
        "\n",
        "### üìñ Key Points\n",
        "\n",
        "1. **SparkSession** is the single entry point to Spark (replaces the older SparkContext and SQLContext)\n",
        "2. `local[*]` means use all available CPU cores ‚Äî great for development\n",
        "3. The **Spark UI** is invaluable for debugging and understanding performance\n",
        "\n",
        "**Note for Colab users:** The Spark UI link (localhost:4040) won't work in Colab ‚Äî that's expected."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3291872a",
      "metadata": {
        "id": "3291872a"
      },
      "source": [
        "### üß† Can Everything Be Parallelized?\n",
        "\n",
        "A key conceptual question from the learning lab: **If parallelism is so powerful, why not always use Spark?**\n",
        "\n",
        "**The Answer:**\n",
        "\n",
        "1. **Not all problems are \"embarrassingly parallel\"**\n",
        "   - Word count: ‚úÖ Each document is independent\n",
        "   - Sorting: ‚ùå Need to see all elements\n",
        "   - Iterative algorithms: ‚ùå Each step depends on the previous\n",
        "\n",
        "2. **The Shuffle is Expensive**\n",
        "   - In a cluster, shuffle = sending data across the network\n",
        "   - Networks are 100-1000x slower than RAM\n",
        "\n",
        "3. **Amdahl's Law**\n",
        "   - If 10% of your task is sequential, you can never get more than 10x speedup\n",
        "   - Even with infinite parallel resources!\n",
        "\n",
        "4. **Coordination Overhead**\n",
        "   - Tracking data locations, detecting failures, collecting results\n",
        "   - For small tasks, overhead exceeds the work itself\n",
        "\n",
        "**Practical Wisdom:**\n",
        "\n",
        "| Data Size | Best Tool |\n",
        "|-----------|-----------|\n",
        "| < 1 GB | Pandas |\n",
        "| 1-100 GB | Spark (single machine) |\n",
        "| 100+ GB | Spark (cluster) |\n",
        "| Real-time | Streaming (Kafka, Flink) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d40604e",
      "metadata": {
        "id": "5d40604e"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LearningLab-Solutions\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"‚úì Spark version: {spark.version}\")\n",
        "print(f\"‚úì Using {spark.sparkContext.defaultParallelism} CPU cores\")\n",
        "print(f\"\\nüåê Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(\"\\nüî• Spark is ready to go!\")\n",
        "print(\"\\n   (Colab users: The Spark UI link won't work ‚Äî that's expected. Skip UI steps.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073f209b",
      "metadata": {
        "id": "073f209b"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 4-5: Pandas to Spark DataFrames\n",
        "\n",
        "### üìñ Key Syntax Differences\n",
        "\n",
        "| Pandas | Spark | Note |\n",
        "|--------|-------|------|\n",
        "| `df.head()` | `df.show()` | Spark returns None, prints to stdout |\n",
        "| `df[['col']]` | `df.select('col')` | Spark uses method chaining |\n",
        "| `df[df['x'] > 5]` | `df.filter(df['x'] > 5)` | Very similar! |\n",
        "| `df['new'] = ...` | `df.withColumn(...)` | Spark is immutable |\n",
        "| `len(df)` | `df.count()` | count() triggers computation! |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df998d1",
      "metadata": {
        "id": "8df998d1"
      },
      "outputs": [],
      "source": [
        "# Create sample data\n",
        "np.random.seed(42)\n",
        "n_rows = 100_000\n",
        "\n",
        "cities = ['Stockholm', 'Gothenburg', 'Malm√∂', 'Uppsala', 'Lund',\n",
        "          'V√§ster√•s', '√ñrebro', 'Link√∂ping', 'Helsingborg', 'J√∂nk√∂ping']\n",
        "\n",
        "large_data = {\n",
        "    'id': range(n_rows),\n",
        "    'age': np.random.randint(18, 70, n_rows),\n",
        "    'city': np.random.choice(cities, n_rows),\n",
        "    'salary': np.random.randint(30000, 100000, n_rows),\n",
        "    'years_experience': np.random.randint(0, 40, n_rows)\n",
        "}\n",
        "\n",
        "df_large_pandas = pd.DataFrame(large_data)\n",
        "df_large_spark = spark.createDataFrame(df_large_pandas)\n",
        "\n",
        "print(f\"Created dataset: {n_rows:,} rows\")\n",
        "print(f\"Partitions: {df_large_spark.rdd.getNumPartitions()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b54a7a",
      "metadata": {
        "id": "e1b54a7a"
      },
      "source": [
        "---\n",
        "\n",
        "## üèãÔ∏è Exercise B: Partition Experiment ‚Äî SOLUTIONS\n",
        "\n",
        "### üìñ Understanding Partitions\n",
        "\n",
        "**Why partitions matter:**\n",
        "- Too few ‚Üí not enough parallelism, CPU cores sit idle\n",
        "- Too many ‚Üí scheduling overhead, many small tasks\n",
        "- Rule of thumb: 2-4 partitions per core for CPU-bound work\n",
        "\n",
        "**repartition() vs coalesce():**\n",
        "- `repartition(n)` ‚Äî full shuffle, can increase or decrease partition count\n",
        "- `coalesce(n)` ‚Äî no shuffle, can only decrease (more efficient for reducing partitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ade70b5",
      "metadata": {
        "id": "4ade70b5"
      },
      "outputs": [],
      "source": [
        "# SOLUTION: Partition Experiment\n",
        "\n",
        "n_rows_experiment = 1_000_000\n",
        "\n",
        "experiment_data = {\n",
        "    'id': range(n_rows_experiment),\n",
        "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows_experiment),\n",
        "    'value': np.random.randint(1, 1000, n_rows_experiment),\n",
        "}\n",
        "\n",
        "df_experiment_pandas = pd.DataFrame(experiment_data)\n",
        "df_experiment = spark.createDataFrame(df_experiment_pandas)\n",
        "\n",
        "print(f\"Dataset: {n_rows_experiment:,} rows\")\n",
        "print(f\"Default partitions: {df_experiment.rdd.getNumPartitions()}\")\n",
        "print(f\"Available cores: {spark.sparkContext.defaultParallelism}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae29957",
      "metadata": {
        "id": "eae29957"
      },
      "outputs": [],
      "source": [
        "def time_aggregation(df, label):\n",
        "    \"\"\"Time a groupBy aggregation.\"\"\"\n",
        "    start = time.time()\n",
        "    result = df.groupBy('category').agg(\n",
        "        F.avg('value').alias('avg'),\n",
        "        F.sum('value').alias('sum'),\n",
        "        F.count('value').alias('count')\n",
        "    ).collect()\n",
        "    duration = time.time() - start\n",
        "    print(f\"{label}: {duration*1000:.0f} ms ({df.rdd.getNumPartitions()} partitions)\")\n",
        "    return duration\n",
        "\n",
        "# Run experiments\n",
        "print(\"\\n‚è±Ô∏è Timing experiments:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "time_default = time_aggregation(df_experiment, \"Default\")\n",
        "\n",
        "df_2 = df_experiment.coalesce(2)\n",
        "time_2 = time_aggregation(df_2, \"2 partitions\")\n",
        "\n",
        "df_100 = df_experiment.repartition(100)\n",
        "time_100 = time_aggregation(df_100, \"100 partitions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864bb89e",
      "metadata": {
        "id": "864bb89e"
      },
      "outputs": [],
      "source": [
        "# SOLUTION: Analysis\n",
        "print(\"\\nüìä Performance Summary:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Default ({df_experiment.rdd.getNumPartitions()} partitions): {time_default*1000:.0f} ms\")\n",
        "print(f\"2 partitions:   {time_2*1000:.0f} ms\")\n",
        "print(f\"100 partitions: {time_100*1000:.0f} ms\")\n",
        "\n",
        "print(\"\\n‚úÖ EXPECTED OBSERVATIONS:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"‚Ä¢ 2 partitions: Often SLOWER (only 2 tasks, other cores idle)\")\n",
        "print(\"‚Ä¢ 100 partitions: May be SLOWER (scheduling overhead, small tasks)\")\n",
        "print(f\"‚Ä¢ Sweet spot: Usually around {spark.sparkContext.defaultParallelism * 2}-{spark.sparkContext.defaultParallelism * 4} partitions\")\n",
        "print(\"\\nüí° Key insight: More partitions ‚â† always better!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159f73cb",
      "metadata": {
        "id": "159f73cb"
      },
      "source": [
        "### üí° Discussion Answers - Exercise B\n",
        "\n",
        "**Q: What happens with too few partitions?**\n",
        "- A: Cores sit idle. With 2 partitions on 8 cores, 6 cores do nothing.\n",
        "\n",
        "**Q: What happens with too many partitions?**\n",
        "- A: Scheduling overhead. Each task has startup cost. With 1000 tiny tasks, you spend more time scheduling than computing.\n",
        "\n",
        "**Q: When might you want MORE partitions than cores?**\n",
        "- A: When data is skewed! If one partition has 90% of the data, you want to split it further so the work is balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34abe663",
      "metadata": {
        "id": "34abe663"
      },
      "source": [
        "---\n",
        "\n",
        "## üèãÔ∏è Exercise C: Spark UI Scavenger Hunt ‚Äî SOLUTIONS\n",
        "\n",
        "### üìñ What to Look For in the Spark UI\n",
        "\n",
        "After running an aggregation, open the Spark UI:\n",
        "\n",
        "| Item | Where to Find | What You'll See |\n",
        "|------|---------------|----------------|\n",
        "| Jobs | Jobs tab | 1-2 jobs per query |\n",
        "| Stages | Click on a job | 2 stages (read ‚Üí shuffle ‚Üí aggregate) |\n",
        "| Tasks | Click on a stage | # tasks = # partitions |\n",
        "| Shuffle | Stage details | Shuffle Read/Write in bytes |\n",
        "| Timeline | Event Timeline | Colored bars showing parallel execution |\n",
        "\n",
        "### üìä Key Visualizations\n",
        "\n",
        "**Event Timeline** (in stage details):\n",
        "- Shows tasks as horizontal bars on a timeline\n",
        "- Overlapping bars = parallel execution\n",
        "- Gaps = waiting (shuffle, scheduling)\n",
        "\n",
        "**DAG Visualization** (click \"DAG Visualization\"):\n",
        "- Shows how stages connect and depend on each other\n",
        "- Arrows between stages = shuffles (data movement)\n",
        "- Visual version of `.explain()` output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aa925a4",
      "metadata": {
        "id": "2aa925a4"
      },
      "outputs": [],
      "source": [
        "# Run the scavenger hunt query\n",
        "scavenger_result = df_large_spark.groupBy('city').agg(\n",
        "    F.avg('salary').alias('avg_salary'),\n",
        "    F.max('years_experience').alias('max_experience'),\n",
        "    F.count('*').alias('employee_count')\n",
        ").orderBy(F.desc('avg_salary'))\n",
        "\n",
        "scavenger_result.show()\n",
        "\n",
        "print(f\"\\nüåê Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(\"\\nGo to Jobs tab ‚Üí Click latest job ‚Üí Explore stages!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c92197",
      "metadata": {
        "id": "40c92197"
      },
      "outputs": [],
      "source": [
        "# For Colab users: Examine the query plan\n",
        "print(\"Query Plan (Physical):\")\n",
        "print(\"=\" * 60)\n",
        "scavenger_result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad837925",
      "metadata": {
        "id": "ad837925"
      },
      "source": [
        "### üìñ How to Read the Query Plan\n",
        "\n",
        "The output above shows **4 different views** of the same query, from abstract to concrete:\n",
        "\n",
        "**1. Parsed Logical Plan** ‚Äî What you wrote, parsed into a tree structure\n",
        "- Just translates your code, no optimizations yet\n",
        "\n",
        "**2. Analyzed Logical Plan** ‚Äî Same as above, but with resolved column types\n",
        "- Now Spark knows `avg_salary` is a `double`, `employee_count` is a `bigint`, etc.\n",
        "\n",
        "**3. Optimized Logical Plan** ‚Äî Spark's optimizer has improved your query!\n",
        "- Notice the `Project` step ‚Äî Spark realized it only needs 3 columns (`city`, `salary`, `years_experience`), so it drops `id` and `age` early to save memory\n",
        "\n",
        "**4. Physical Plan** ‚Äî The actual execution strategy (this is what runs!)\n",
        "\n",
        "### üîç Reading the Physical Plan (Bottom to Top!)\n",
        "\n",
        "Read from the **bottom up** ‚Äî that's the order of execution:\n",
        "\n",
        "```\n",
        "Scan ExistingRDD           ‚Üê 1. Read the data\n",
        "   ‚Üì\n",
        "Project [city, salary, years_experience]  ‚Üê 2. Keep only needed columns\n",
        "   ‚Üì\n",
        "HashAggregate (partial)    ‚Üê 3. Partial aggregation BEFORE shuffle (optimization!)\n",
        "   ‚Üì\n",
        "Exchange hashpartitioning  ‚Üê 4. SHUFFLE! Send data to reducers by city\n",
        "   ‚Üì\n",
        "HashAggregate (final)      ‚Üê 5. Final aggregation after shuffle\n",
        "   ‚Üì\n",
        "Exchange rangepartitioning ‚Üê 6. Another shuffle for sorting\n",
        "   ‚Üì\n",
        "Sort                       ‚Üê 7. Sort by avg_salary descending\n",
        "```\n",
        "\n",
        "### üéØ Key Insights\n",
        "\n",
        "- **Two shuffles!** (`Exchange` = shuffle). One for grouping by city, one for sorting.\n",
        "- **Partial aggregation**: Spark computes partial averages/counts *before* shuffling (see `partial_avg`, `partial_count`). This reduces network traffic ‚Äî smart!\n",
        "- **AdaptiveSparkPlan**: Spark can adjust the plan at runtime based on actual data sizes.\n",
        "- **200 partitions**: The default shuffle partition count (you can tune this with `spark.sql.shuffle.partitions`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c523783b",
      "metadata": {
        "id": "c523783b"
      },
      "source": [
        "### ‚úÖ Scavenger Hunt Answers\n",
        "\n",
        "| Question | Typical Answer | Explanation |\n",
        "|----------|---------------|-------------|\n",
        "| How many jobs? | 1-2 | One for the aggregation, sometimes one for show() |\n",
        "| Stages per job? | 2 | Stage 1: Partial aggregate, Stage 2: Final aggregate |\n",
        "| Tasks per stage? | = partition count | One task per partition |\n",
        "| Concurrent execution? | Yes | Look at the timeline ‚Äî parallel colored bars |\n",
        "| Shuffle data size? | Few KB | Aggregates are small (just counts/sums per city) |\n",
        "\n",
        "**Why 2 stages?**\n",
        "- Stage 1: Each partition computes partial aggregates (local sum, count)\n",
        "- Shuffle: Send partial results to reducer\n",
        "- Stage 2: Combine partial results into final aggregates\n",
        "\n",
        "This is the same Map-Shuffle-Reduce pattern we saw earlier!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d65b40",
      "metadata": {
        "id": "50d65b40"
      },
      "source": [
        "---\n",
        "\n",
        "## üèãÔ∏è Exercise D: Distributed Thinking ‚Äî SOLUTIONS\n",
        "\n",
        "### Scenario 1: Web Log Analysis (500 GB)\n",
        "\n",
        "**Q1: Would this fit in Pandas?**\n",
        "- A: No! 500 GB requires ~500 GB RAM (plus overhead)\n",
        "- Most laptops have 8-32 GB RAM\n",
        "- You'd need a very expensive machine or Spark on a cluster\n",
        "\n",
        "**Q2: How many partitions for a 10-node cluster?**\n",
        "- Rule of thumb: 2-4 partitions per core\n",
        "- If each node has 8 cores: 10 √ó 8 √ó 3 ‚âà 240 partitions\n",
        "- Or: 500 GB / 128 MB per partition ‚âà 4000 partitions\n",
        "- The larger number is often better for large data\n",
        "\n",
        "**Q3: What if one file is 400 GB?**\n",
        "- This is **data skew**!\n",
        "- One partition (the 400 GB file) will take forever\n",
        "- Others finish quickly, then wait\n",
        "- Solution: Repartition the data, or use salting techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015127cd",
      "metadata": {
        "id": "015127cd"
      },
      "source": [
        "### Scenario 2: Data Pipeline Failure\n",
        "\n",
        "**Q1: What is this symptom called?**\n",
        "- A: **Data skew** or **straggler task**\n",
        "- One task has much more data than others\n",
        "\n",
        "**Q2: What might cause it?**\n",
        "- Uneven data distribution (e.g., 90% of orders from one customer)\n",
        "- Join on a skewed key\n",
        "- One corrupt/huge input file\n",
        "- Null values all going to one partition\n",
        "\n",
        "**Q3: Investigation and fix:**\n",
        "1. Check Spark UI ‚Üí Stages ‚Üí Look for uneven task durations\n",
        "2. Check input data distribution\n",
        "3. Fixes:\n",
        "   - Salting: Add random prefix to skewed keys\n",
        "   - Broadcast join for small tables\n",
        "   - Filter out bad data before processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6340ef4f",
      "metadata": {
        "id": "6340ef4f"
      },
      "source": [
        "### Scenario 3: Real-time vs Batch\n",
        "\n",
        "**Trade-offs:**\n",
        "\n",
        "| Aspect | Batch | Real-time |\n",
        "|--------|-------|----------|\n",
        "| Latency | Hours | Seconds |\n",
        "| Complexity | Simpler | More complex |\n",
        "| Cost | Cheaper (can use spot instances) | More expensive (always on) |\n",
        "| Fault tolerance | Easy (restart job) | Harder (need checkpointing) |\n",
        "\n",
        "**Real-time essential:**\n",
        "- Fraud detection (need to block transaction immediately)\n",
        "- Stock trading\n",
        "- Real-time recommendations\n",
        "- Alerting/monitoring\n",
        "\n",
        "**Batch sufficient:**\n",
        "- Daily sales reports\n",
        "- Monthly billing\n",
        "- Training ML models\n",
        "- Historical analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee77632",
      "metadata": {
        "id": "3ee77632"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 8: Lazy Evaluation\n",
        "\n",
        "### üìñ Why Lazy Evaluation is Powerful\n",
        "\n",
        "**1. Optimization:** Spark can see the entire pipeline and optimize it\n",
        "   - Push filters down (filter early = less data to process)\n",
        "   - Combine operations\n",
        "   - Avoid unnecessary shuffles\n",
        "\n",
        "**2. Efficiency:** Spark doesn't compute what you don't need\n",
        "   - If you call `filter().select().take(10)`, why process all rows?\n",
        "\n",
        "**Important:** Transformations like `filter()` don't run immediately ‚Äî nothing happens until you call an action like `show()` or `collect()`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6142bf",
      "metadata": {
        "id": "6a6142bf"
      },
      "outputs": [],
      "source": [
        "# Demonstrating lazy evaluation\n",
        "start = time.time()\n",
        "\n",
        "# This is INSTANT - just building a plan\n",
        "result = df_large_spark \\\n",
        "    .filter(df_large_spark['age'] > 30) \\\n",
        "    .filter(df_large_spark['salary'] > 50000) \\\n",
        "    .select('city', 'salary') \\\n",
        "    .groupBy('city') \\\n",
        "    .agg(F.avg('salary').alias('avg_salary'))\n",
        "\n",
        "planning_time = time.time() - start\n",
        "print(f\"‚ö° Planning took: {planning_time*1000:.2f} ms\")\n",
        "print(f\"   Nothing computed yet! Just built a plan.\")\n",
        "print(f\"\\nQuery plan:\")\n",
        "result.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1275a2ab",
      "metadata": {
        "id": "1275a2ab"
      },
      "outputs": [],
      "source": [
        "# Now trigger execution\n",
        "start = time.time()\n",
        "actual_result = result.collect()\n",
        "execution_time = time.time() - start\n",
        "\n",
        "print(f\"‚è±Ô∏è Execution took: {execution_time*1000:.2f} ms\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea14df49",
      "metadata": {
        "id": "ea14df49"
      },
      "source": [
        "---\n",
        "\n",
        "## Wrap-up & Key Takeaways\n",
        "\n",
        "### üìñ What You Learned Today\n",
        "\n",
        "1. **Environment setup** ‚Äî How to verify and configure PySpark\n",
        "2. **MapReduce** ‚Äî The foundational paradigm (Map ‚Üí Shuffle ‚Üí Reduce)\n",
        "3. **SparkSession** ‚Äî The entry point to Spark functionality\n",
        "4. **MapReduce vs Spark** ‚Äî Same word count in 5 lines instead of 30!\n",
        "5. **Pandas ‚Üí Spark** ‚Äî Converting DataFrames and the similar syntax\n",
        "6. **Scaling** ‚Äî Why Spark matters for large datasets\n",
        "7. **Partitions** ‚Äî How Spark distributes data for parallel processing\n",
        "8. **Lazy evaluation** ‚Äî Transformations vs Actions\n",
        "9. **Spark UI** ‚Äî Monitoring what Spark is doing\n",
        "10. **Distributed thinking** ‚Äî Reasoning about data at scale\n",
        "\n",
        "### Key Concepts to Remember\n",
        "\n",
        "- **MapReduce** is the foundational pattern: Map (transform) ‚Üí Shuffle (group) ‚Üí Reduce (combine)\n",
        "- Spark is designed for **distributed** computing ‚Äî it shines with big data\n",
        "- Spark keeps data **in memory** instead of writing to disk like Hadoop MapReduce\n",
        "- **Lazy evaluation** lets Spark optimize your entire pipeline\n",
        "- **Partitions** determine parallelism ‚Äî balance is key\n",
        "- **Data skew** (uneven partitions) is a common performance killer\n",
        "- The **Spark UI** is your best friend for debugging and optimization\n",
        "\n",
        "### Common Pitfalls to Avoid\n",
        "\n",
        "- Collecting too much data to the driver (`collect()` on large data)\n",
        "- Assuming `filter()` runs immediately (it doesn't ‚Äî it's lazy!)\n",
        "- Ignoring data skew\n",
        "- Using the wrong partition count\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "- **Next week (Week 5):** Lab 1 will be published ‚Äî your first graded assignment!\n",
        "- Lab 1 dives deeper into transformations, actions, and the execution model\n",
        "- You'll work with larger datasets and submit your completed notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c785f6ca",
      "metadata": {
        "id": "c785f6ca"
      },
      "outputs": [],
      "source": [
        "# Clean up\n",
        "spark.stop()\n",
        "print(\"‚úì Spark session stopped.\")\n",
        "print(\"\\nüéâ Learning Lab Solutions Complete!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}