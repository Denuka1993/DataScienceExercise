{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denuka1993/DataScienceExercise/blob/main/learning_lab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59181ec4",
      "metadata": {
        "id": "59181ec4"
      },
      "source": [
        "# Learning Lab: Getting Started with Big Data Tools\n",
        "\n",
        "**Course:** GIK2Q3 Applied Big Data and Cloud Computing  \n",
        "**Duration:** ~3 hours  \n",
        "**Week:** 4\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“˜ This is a Learning Lab â€” Not Graded!\n",
        "\n",
        "Use this session to explore the tools and get comfortable with the environment. There's nothing to submit â€” just run the cells, experiment, and ask questions.\n",
        "\n",
        "**Lab 1** (your first graded assignment) will be published next week and will build on what you learn here.\n",
        "\n",
        "---\n",
        "\n",
        "**Objectives:**\n",
        "- Verify your environment is working\n",
        "- Run your first PySpark code\n",
        "- Explore the differences between Pandas and PySpark\n",
        "- Understand why Spark matters for big data\n",
        "- Explore the Spark UI to see what's happening behind the scenes\n",
        "- Get comfortable with Jupyter notebooks for big data work\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ Important: Google Colab Fallback\n",
        "\n",
        "If you have trouble running PySpark locally (especially on Windows), you can use **Google Colab** as a backup:\n",
        "\n",
        "1. Upload this notebook to [Google Colab](https://colab.research.google.com/)\n",
        "2. Colab has Java pre-installed, so PySpark will work out of the box\n",
        "3. **Note:** Spark UI links (localhost:4040) won't work in Colab â€” that's expected. Skip those exploration steps if using Colab.\n",
        "\n",
        "The cells below will auto-detect your environment and install PySpark if needed.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ Note: BrokenPipeError Messages\n",
        "\n",
        "You may see `BrokenPipeError: [Errno 32] Broken pipe` messages after some cells complete. **These are harmless** â€” your code worked fine! This is a known PySpark quirk where worker subprocesses try to flush output after the parent has already closed the connection.\n",
        "\n",
        "**How to tell if your code worked:** If you see the expected output *before* the error message, everything ran correctly. Just ignore the traceback."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c411620",
      "metadata": {
        "id": "9c411620"
      },
      "source": [
        "## Section 1: Environment Check\n",
        "\n",
        "Let's make sure everything is installed and working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e917b4e",
      "metadata": {
        "id": "1e917b4e"
      },
      "outputs": [],
      "source": [
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b738f653",
      "metadata": {
        "id": "b738f653"
      },
      "outputs": [],
      "source": [
        "# Install PySpark if needed (mainly for Google Colab)\n",
        "try:\n",
        "    import pyspark\n",
        "    print(f\"âœ“ PySpark already installed: version {pyspark.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Installing PySpark...\")\n",
        "    %pip install pyspark -q\n",
        "    import pyspark\n",
        "    print(f\"âœ“ PySpark installed: version {pyspark.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc25cd3f",
      "metadata": {
        "id": "fc25cd3f"
      },
      "outputs": [],
      "source": [
        "# Import other libraries we'll use\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(f\"âœ“ Pandas version: {pd.__version__}\")\n",
        "print(f\"âœ“ NumPy version: {np.__version__}\")\n",
        "print(\"\\nðŸŽ‰ All libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c03aa2a",
      "metadata": {
        "id": "0c03aa2a"
      },
      "outputs": [],
      "source": [
        "# Environment configuration for local installations\n",
        "import os\n",
        "\n",
        "# Set JAVA_HOME for local installations (macOS/Linux)\n",
        "# This is only needed for local development - Colab has Java pre-installed\n",
        "java_paths = [\n",
        "    \"/opt/homebrew/opt/openjdk@17\",  # macOS Homebrew (Apple Silicon)\n",
        "    \"/usr/local/opt/openjdk@17\",      # macOS Homebrew (Intel)\n",
        "    \"/usr/lib/jvm/java-17-openjdk-amd64\",  # Ubuntu/Debian\n",
        "]\n",
        "\n",
        "java_home = os.environ.get(\"JAVA_HOME\")\n",
        "\n",
        "if not java_home:\n",
        "    for path in java_paths:\n",
        "        if os.path.exists(path):\n",
        "            os.environ[\"JAVA_HOME\"] = path\n",
        "            java_home = path\n",
        "            break\n",
        "\n",
        "if java_home:\n",
        "    print(f\"âœ“ JAVA_HOME set to: {java_home}\")\n",
        "else:\n",
        "    print(\"âš ï¸ Java not found. PySpark requires Java 17.\")\n",
        "    print(\"  On macOS: brew install openjdk@17\")\n",
        "    print(\"  On Ubuntu: sudo apt install openjdk-17-jdk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4e4f36f",
      "metadata": {
        "id": "c4e4f36f"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: Understanding MapReduce\n",
        "\n",
        "Before we dive into Spark, let's understand the paradigm that started it all: **MapReduce**.\n",
        "\n",
        "MapReduce was introduced by Google in 2004 and became the foundation for processing massive datasets across clusters of computers. The key insight is simple:\n",
        "\n",
        "1. **Map**: Transform each piece of data independently (can run in parallel)\n",
        "2. **Reduce**: Combine the results together\n",
        "\n",
        "This pattern is powerful because the Map step can run on thousands of machines simultaneously!\n",
        "\n",
        "### The Classic Example: Word Count\n",
        "\n",
        "Let's count words in a text â€” first with pure Python to understand the concept, then we'll see how Spark makes it easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0f7f68d",
      "metadata": {
        "id": "c0f7f68d"
      },
      "outputs": [],
      "source": [
        "# Sample text data - imagine this is a huge file distributed across many machines\n",
        "text_data = \"\"\"\n",
        "Big data is transforming how we understand the world\n",
        "Data science and machine learning rely on big data\n",
        "The world of data is growing every day\n",
        "Machine learning models need data to learn\n",
        "Big models require big data and big compute\n",
        "\"\"\"\n",
        "\n",
        "# Split into lines (simulating data distributed across machines)\n",
        "lines = text_data.strip().split('\\n')\n",
        "print(f\"We have {len(lines)} 'documents' to process:\")\n",
        "for i, line in enumerate(lines):\n",
        "    print(f\"  Doc {i}: {line[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e6ea843",
      "metadata": {
        "id": "7e6ea843"
      },
      "source": [
        "### Step 1: The MAP Phase\n",
        "\n",
        "The **map** function processes each document independently and emits (key, value) pairs.\n",
        "\n",
        "For word count: `\"hello world\"` â†’ `[(\"hello\", 1), (\"world\", 1)]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "738aca0c",
      "metadata": {
        "id": "738aca0c"
      },
      "outputs": [],
      "source": [
        "def map_function(document):\n",
        "    \"\"\"\n",
        "    MAP: Takes a document, emits (word, 1) for each word.\n",
        "    This function can run independently on each machine!\n",
        "    \"\"\"\n",
        "    words = document.lower().split()\n",
        "    # Emit (key, value) pairs\n",
        "    return [(word, 1) for word in words]\n",
        "\n",
        "# Apply map to each document\n",
        "mapped_results = []\n",
        "for doc in lines:\n",
        "    result = map_function(doc)\n",
        "    mapped_results.extend(result)\n",
        "    print(f\"Map output: {result[:3]}...\")  # Show first 3 pairs\n",
        "\n",
        "print(f\"\\nðŸ“¤ Total (word, 1) pairs emitted: {len(mapped_results)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044c470c",
      "metadata": {
        "id": "044c470c"
      },
      "source": [
        "### Step 2: The SHUFFLE Phase\n",
        "\n",
        "Before reducing, we need to group all values by key. This is the **shuffle** â€” moving data between machines so all pairs with the same key end up together.\n",
        "\n",
        "`[(\"big\", 1), (\"data\", 1), (\"big\", 1)]` â†’ `{\"big\": [1, 1], \"data\": [1]}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b0501a",
      "metadata": {
        "id": "12b0501a"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def shuffle(mapped_pairs):\n",
        "    \"\"\"\n",
        "    SHUFFLE: Group all values by key.\n",
        "    In a real cluster, this moves data across the network!\n",
        "    \"\"\"\n",
        "    grouped = defaultdict(list)\n",
        "    for key, value in mapped_pairs:\n",
        "        grouped[key].append(value)\n",
        "    return grouped\n",
        "\n",
        "shuffled = shuffle(mapped_results)\n",
        "print(\"After shuffle - values grouped by key:\")\n",
        "for word in list(shuffled.keys())[:5]:  # Show first 5 words\n",
        "    print(f\"  '{word}': {shuffled[word]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99fb9999",
      "metadata": {
        "id": "99fb9999"
      },
      "source": [
        "### Step 3: The REDUCE Phase\n",
        "\n",
        "The **reduce** function takes each key and its list of values, and combines them into a final result.\n",
        "\n",
        "`(\"big\", [1, 1, 1])` â†’ `(\"big\", 3)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f5a4ad",
      "metadata": {
        "id": "85f5a4ad"
      },
      "outputs": [],
      "source": [
        "def reduce_function(key, values):\n",
        "    \"\"\"\n",
        "    REDUCE: Combine all values for a key.\n",
        "    This can also run in parallel â€” one reducer per key (or group of keys)!\n",
        "    \"\"\"\n",
        "    return (key, sum(values))\n",
        "\n",
        "# Apply reduce to each key\n",
        "final_counts = {}\n",
        "for word, counts in shuffled.items():\n",
        "    key, total = reduce_function(word, counts)\n",
        "    final_counts[key] = total\n",
        "\n",
        "# Sort by count and display\n",
        "sorted_counts = sorted(final_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"ðŸŽ¯ Final word counts (MapReduce result):\")\n",
        "print(\"-\" * 30)\n",
        "for word, count in sorted_counts[:10]:\n",
        "    bar = 'â–ˆ' * count\n",
        "    print(f\"  {word:12} : {count:2} {bar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c821fee7",
      "metadata": {
        "id": "c821fee7"
      },
      "source": [
        "### Visualizing the MapReduce Flow\n",
        "\n",
        "```\n",
        "    Document 1         Document 2         Document 3\n",
        "        â”‚                  â”‚                  â”‚\n",
        "        â–¼                  â–¼                  â–¼\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚  MAP  â”‚          â”‚  MAP  â”‚          â”‚  MAP  â”‚   â† Parallel!\n",
        "    â””â”€â”€â”€â”¬â”€â”€â”€â”˜          â””â”€â”€â”€â”¬â”€â”€â”€â”˜          â””â”€â”€â”€â”¬â”€â”€â”€â”˜\n",
        "        â”‚                  â”‚                  â”‚\n",
        "        â–¼                  â–¼                  â–¼\n",
        "    (big,1)            (data,1)           (big,1)\n",
        "    (data,1)           (is,1)             (models,1)\n",
        "       ...                ...                ...\n",
        "        â”‚                  â”‚                  â”‚\n",
        "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                           â”‚\n",
        "                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”\n",
        "                    â”‚   SHUFFLE   â”‚   â† Network transfer\n",
        "                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
        "                           â”‚\n",
        "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "            â–¼              â–¼              â–¼\n",
        "    big: [1,1,1]    data: [1,1,1,1]  models: [1,1]\n",
        "            â”‚              â”‚              â”‚\n",
        "            â–¼              â–¼              â–¼\n",
        "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n",
        "        â”‚REDUCE â”‚      â”‚REDUCE â”‚      â”‚REDUCE â”‚   â† Parallel!\n",
        "        â””â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”˜\n",
        "            â”‚              â”‚              â”‚\n",
        "            â–¼              â–¼              â–¼\n",
        "        (big, 3)       (data, 4)     (models, 2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e5bcee",
      "metadata": {
        "id": "03e5bcee"
      },
      "source": [
        "### Why MapReduce Changed Everything\n",
        "\n",
        "**Before MapReduce (2004):**\n",
        "- Processing large datasets required specialized supercomputers\n",
        "- Scaling was expensive and complex\n",
        "\n",
        "**After MapReduce:**\n",
        "- Use thousands of cheap commodity machines\n",
        "- Fault tolerance built-in (if a machine fails, just re-run that map task)\n",
        "- Google used this to build their search index!\n",
        "\n",
        "**But MapReduce has limitations:**\n",
        "- Writes intermediate data to disk between each step (slow!)\n",
        "- Not great for iterative algorithms (machine learning, graph processing)\n",
        "- Verbose to program\n",
        "\n",
        "**Enter Spark (2014):**\n",
        "- Keeps data in memory between operations\n",
        "- 10-100x faster than MapReduce for many workloads\n",
        "- Much easier API (as we'll see next!)\n",
        "\n",
        "Let's now see how Spark makes this much simpler..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ecc3e4",
      "metadata": {
        "id": "c2ecc3e4"
      },
      "outputs": [],
      "source": [
        "# Challenge 1: Count only words with more than 3 characters\n",
        "# Your solution here:\n",
        "\n",
        "def map_function_filtered(document):\n",
        "    \"\"\"MAP: Emit (word, 1) only for words with more than 3 characters.\"\"\"\n",
        "    words = document.lower().split()\n",
        "    # TODO: Add a filter condition\n",
        "    return [(word, 1) for word in words]  # Modify this line\n",
        "\n",
        "# Test your solution\n",
        "mapped_filtered = []\n",
        "for doc in lines:\n",
        "    mapped_filtered.extend(map_function_filtered(doc))\n",
        "\n",
        "# Shuffle and reduce (same as before)\n",
        "shuffled_filtered = shuffle(mapped_filtered)\n",
        "filtered_counts = {k: sum(v) for k, v in shuffled_filtered.items()}\n",
        "print(\"Words with >3 characters:\")\n",
        "for word, count in sorted(filtered_counts.items(), key=lambda x: -x[1])[:8]:\n",
        "    print(f\"  {word}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef97fe7e",
      "metadata": {
        "id": "ef97fe7e"
      },
      "outputs": [],
      "source": [
        "# Challenge 3: Find the longest word using MapReduce pattern\n",
        "# Your solution here:\n",
        "\n",
        "# Hint: What should map emit? What should reduce do?\n",
        "# Think about it: map could emit (1, word) where 1 is a dummy key\n",
        "# reduce could compare two words and keep the longer one\n",
        "\n",
        "def map_longest(document):\n",
        "    \"\"\"MAP: Emit the longest word from this document.\"\"\"\n",
        "    words = document.split()\n",
        "    if not words:\n",
        "        return []\n",
        "    # TODO: Return the longest word from this document\n",
        "    return [(\"longest\", max(words, key=len))]  # What key should we use?\n",
        "\n",
        "def reduce_longest(word1, word2):\n",
        "    \"\"\"REDUCE: Compare two words, keep the longer one.\"\"\"\n",
        "    # TODO: Return the longer word\n",
        "    pass\n",
        "\n",
        "# Test your solution - discuss with your group:\n",
        "# 1. Is this truly parallelizable?\n",
        "# 2. What happens if two words have the same length?\n",
        "# 3. How would you handle ties (return all longest words)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e88c2cc",
      "metadata": {
        "id": "3e88c2cc"
      },
      "outputs": [],
      "source": [
        "# Challenge 2: Count words starting with capital letters\n",
        "# Your solution here:\n",
        "\n",
        "# Use the original text (not lowercased)\n",
        "original_text = \"\"\"\n",
        "Big data is transforming how we understand the world\n",
        "Data science and machine learning rely on big data\n",
        "The world of data is growing every day\n",
        "Machine learning models need data to learn\n",
        "Big models require big data and big compute\n",
        "\"\"\"\n",
        "\n",
        "def map_capitals(document):\n",
        "    \"\"\"MAP: Emit (word, 1) only for words starting with a capital letter.\"\"\"\n",
        "    words = document.split()\n",
        "    # TODO: Check if word starts with capital\n",
        "    return [(word, 1) for word in words]  # Modify this line\n",
        "\n",
        "# Test your solution\n",
        "original_lines = original_text.strip().split('\\n')\n",
        "mapped_capitals = []\n",
        "for doc in original_lines:\n",
        "    mapped_capitals.extend(map_capitals(doc))\n",
        "\n",
        "shuffled_capitals = shuffle(mapped_capitals)\n",
        "capital_counts = {k: sum(v) for k, v in shuffled_capitals.items()}\n",
        "print(\"Capital words:\")\n",
        "for word, count in sorted(capital_counts.items(), key=lambda x: -x[1])[:8]:\n",
        "    print(f\"  {word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad17045",
      "metadata": {
        "id": "3ad17045"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ‹ï¸ Exercise A: MapReduce Challenge\n",
        "\n",
        "Now that you understand the MapReduce pattern, it's time to apply it! Work in your group to solve these challenges.\n",
        "\n",
        "### Challenge 1: Filter Before Counting\n",
        "Modify the word count to **only count words with more than 3 characters**.\n",
        "\n",
        "*Hint: Add a filter in the map function or between map and reduce.*\n",
        "\n",
        "### Challenge 2: Count Capital Words\n",
        "Count only words that **start with a capital letter** in the original (non-lowercased) text.\n",
        "\n",
        "*Hint: You'll need to modify the map function to check `word[0].isupper()`*\n",
        "\n",
        "### Challenge 3: Find the Longest Word\n",
        "Find the single longest word in the text.\n",
        "\n",
        "*Discussion: Can you parallelize this? What would the map and reduce functions look like?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffde4076",
      "metadata": {
        "id": "ffde4076"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 3: Your First SparkSession\n",
        "\n",
        "Now that you understand MapReduce, let's see how **Spark** makes distributed computing much easier.\n",
        "\n",
        "The **SparkSession** is your entry point to all Spark functionality. Think of it as opening the door to distributed computing.\n",
        "\n",
        "In a real cluster, SparkSession connects to multiple worker machines. Here, we'll run locally â€” but the API is exactly the same!\n",
        "\n",
        "> âš ï¸ **Google Colab Users â€” Important Reminder!**  \n",
        "> The Spark UI link (`localhost:4040`) that appears after running the next cell **will not work in Colab**. This is expected â€” Colab runs on a remote server, so you can't access localhost URLs. When the notebook asks you to explore the Spark UI, just skip those steps. We'll provide alternative cells using `.explain()` for Colab users later in the notebook (see Exercise C).\n",
        "\n",
        "### Key Configuration Options:\n",
        "- `appName`: A name for your application (shows up in the Spark UI)\n",
        "- `master`: Where Spark runs â€” `local[*]` means use all available CPU cores locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94576e63",
      "metadata": {
        "id": "94576e63"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"LearningLab-GettingStarted\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"âœ“ Spark version: {spark.version}\")\n",
        "print(f\"âœ“ Using {spark.sparkContext.defaultParallelism} CPU cores\")\n",
        "print(f\"\\nðŸŒ Spark UI available at: {spark.sparkContext.uiWebUrl}\")\n",
        "print(\"\\n   â†‘ Click this link to see what Spark is doing behind the scenes!\")\n",
        "print(\"\\nðŸ”¥ Spark is ready to go!\")\n",
        "print(\"\\n   (Colab users: The Spark UI link won't work â€” that's expected. Skip UI steps.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27280c5e",
      "metadata": {
        "id": "27280c5e"
      },
      "source": [
        "### ðŸ‘† Take a Moment: Open the Spark UI\n",
        "\n",
        "Click the link above to open the Spark UI in your browser. Keep it open â€” we'll explore it as we run code!\n",
        "\n",
        "The Spark UI shows:\n",
        "- **Jobs**: Operations that have run\n",
        "- **Stages**: How jobs are broken into stages\n",
        "- **Storage**: Cached data\n",
        "- **Executors**: Worker processes (just 1 locally)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3972d39",
      "metadata": {
        "id": "f3972d39"
      },
      "source": [
        "### Word Count in Spark: So Much Simpler!\n",
        "\n",
        "Remember our 30+ lines of MapReduce code? Here's the same thing in Spark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1681b7f",
      "metadata": {
        "id": "a1681b7f"
      },
      "outputs": [],
      "source": [
        "# Word count in Spark - the same operation in just a few lines!\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Use the same text data\n",
        "text_rdd = spark.sparkContext.parallelize(lines)\n",
        "\n",
        "# MapReduce in Spark: flatMap (map) â†’ map â†’ reduceByKey (shuffle + reduce)\n",
        "word_counts_spark = (\n",
        "    text_rdd\n",
        "    .flatMap(lambda line: line.lower().split())  # MAP: split into words\n",
        "    .map(lambda word: (word, 1))                  # MAP: create (word, 1) pairs\n",
        "    .reduceByKey(lambda a, b: a + b)              # SHUFFLE + REDUCE: sum counts\n",
        ")\n",
        "\n",
        "# Collect and display results\n",
        "print(\"ðŸš€ Word count in Spark (same result, much less code!):\")\n",
        "print(\"-\" * 30)\n",
        "for word, count in sorted(word_counts_spark.collect(), key=lambda x: -x[1])[:10]:\n",
        "    bar = 'â–ˆ' * count\n",
        "    print(f\"  {word:12} : {count:2} {bar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f7392a",
      "metadata": {
        "id": "09f7392a"
      },
      "source": [
        "### ðŸ” Your First Job in the Spark UI!\n",
        "\n",
        "You just ran your first Spark job! Open the Spark UI (link from Section 3) and explore what happened:\n",
        "\n",
        "1. **Go to the Jobs tab** â€” This shows all completed jobs. You should see at least one.\n",
        "2. **Click on a specific job** (e.g., \"Job 0\") to see its details:\n",
        "   - **Note:** You're still in the Jobs tab, but now viewing that job's completed stages â€” the tab name doesn't change, which can be confusing!\n",
        "   - **Duration**: How long did it take?\n",
        "   - **Stages**: The job was broken into stages (likely 2 â€” one for the map, one for the reduce)\n",
        "3. **Click on a stage** to drill down further:\n",
        "   - **Tasks**: Individual units of work that ran in parallel\n",
        "   - **Shuffle Read/Write**: Data movement between stages (this is the \"shuffle\" from MapReduce!)\n",
        "\n",
        "### ðŸ¤” Wait â€” Why Are There 10 Tasks for 5 Lines of Text?\n",
        "\n",
        "You might see ~10 completed tasks even though we only have 5 lines of text. What's going on?\n",
        "\n",
        "When you call `parallelize(lines)`, Spark splits the data into **partitions** â€” and by default, it creates one partition per CPU core (check the \"Using X CPU cores\" message from earlier). So if you have 10 cores, you get 10 partitions, hence 10 tasks.\n",
        "\n",
        "With only 5 lines, most partitions are **empty**! The tasks still run, but they have nothing to process. This is the overhead we mentioned â€” for tiny data, Spark does unnecessary work coordinating empty partitions.\n",
        "\n",
        "**Takeaway:** Spark is designed for data much larger than your CPU count. With millions of rows, each partition would have meaningful work to do.\n",
        "\n",
        "### ðŸ“Š Key Visualizations to Explore\n",
        "\n",
        "**Event Timeline** (in the stage details view):\n",
        "- Shows tasks as horizontal bars on a timeline\n",
        "- Overlapping bars = parallel execution\n",
        "- Look for gaps â€” they indicate waiting (for shuffle, scheduling, etc.)\n",
        "\n",
        "**DAG Visualization** (click \"DAG Visualization\" in the job or stage view):\n",
        "- DAG = Directed Acyclic Graph â€” the execution plan\n",
        "- Shows how stages connect and depend on each other\n",
        "- Arrows between stages represent shuffles (data movement)\n",
        "- This is the visual version of what `.explain()` prints as text!\n",
        "\n",
        "### âš¡ Did We Actually Run in Parallel?\n",
        "\n",
        "**Yes!** Look at the **Event Timeline** â€” you'll see multiple tasks running at the same time on different CPU cores.\n",
        "\n",
        "But here's the honest truth: **with only 5 lines of text, parallelism didn't help much.** The overhead of coordinating parallel work is larger than the work itself! You might see tasks completing almost instantly, possibly even sequentially.\n",
        "\n",
        "This is similar to the Pandas vs Spark comparison we'll do later â€” Spark's power shows up with **large data**, not small examples. We're using tiny data here to learn the concepts, but remember: in the real world, you'd have millions of documents, and those tasks would run on hundreds of machines simultaneously.\n",
        "\n",
        "**What you're seeing:**\n",
        "- Each **task** processed one partition of data\n",
        "- The **shuffle** happened between the map and reduce stages (grouping data by word)\n",
        "- All tasks in a stage *can* run **in parallel** â€” and with bigger data, they will!\n",
        "\n",
        "> ðŸ’¡ These visualizations become essential when debugging slow jobs on real clusters!\n",
        "\n",
        "*(Colab users: Skip this â€” use `.explain()` later in Exercise C to see query plans instead.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e971b88b",
      "metadata": {
        "id": "e971b88b"
      },
      "source": [
        "### ðŸ§  A Deeper Question: Can Everything Be Parallelized?\n",
        "\n",
        "We've seen that Spark can split work across many cores (or machines). But this raises a question: **If parallelism is so powerful, why not always use Spark and forget about RAM limits?**\n",
        "\n",
        "The answer reveals fundamental truths about distributed computing:\n",
        "\n",
        "**1. Not all problems are \"embarrassingly parallel\"**\n",
        "\n",
        "Word count is ideal for parallelism â€” each document can be processed independently. But consider:\n",
        "- **Sorting**: You can't know the final position of an element without seeing all elements\n",
        "- **Iterative algorithms**: Each step depends on the previous (e.g., training neural networks)\n",
        "- **Sequential dependencies**: \"Read line 5, but only if line 4 says X\"\n",
        "\n",
        "These require **coordination**, which means communication, which means waiting.\n",
        "\n",
        "**2. The Shuffle is Expensive**\n",
        "\n",
        "Remember the shuffle phase? In a real cluster, that means sending data **across the network** to other machines. Networks are slow compared to RAM â€” often 100-1000x slower. Every shuffle is a potential bottleneck.\n",
        "\n",
        "**3. Amdahl's Law: The Sequential Part Limits You**\n",
        "\n",
        "If 10% of your task must run sequentially, you can never get more than 10x speedup â€” even with infinite parallel resources. That sequential portion becomes the bottleneck.\n",
        "\n",
        "**4. Coordination Overhead**\n",
        "\n",
        "With 1000 machines, someone has to:\n",
        "- Track which machine has which data\n",
        "- Detect failures and reassign work\n",
        "- Collect and combine results\n",
        "\n",
        "This overhead can exceed the actual work for small tasks!\n",
        "\n",
        "**The Practical Wisdom:**\n",
        "\n",
        "| Data Size | Best Tool | Why |\n",
        "|-----------|-----------|-----|\n",
        "| < 1 GB | Pandas | No overhead, all in RAM |\n",
        "| 1-100 GB | Spark (single machine) | Spill to disk when needed |\n",
        "| 100+ GB | Spark (cluster) | Distribute across machines |\n",
        "| Real-time | Streaming (Kafka, Flink) | Can't wait to batch process |\n",
        "\n",
        "**Bottom line:** Parallelism is a tool, not magic. Use the right tool for the job â€” and often, that's still good old single-machine code!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07a88ea",
      "metadata": {
        "id": "e07a88ea"
      },
      "source": [
        "**What just happened?**\n",
        "\n",
        "| Our MapReduce | Spark Equivalent |\n",
        "|---------------|------------------|\n",
        "| `map_function()` | `.flatMap()` + `.map()` |\n",
        "| `shuffle()` | Automatic! |\n",
        "| `reduce_function()` | `.reduceByKey()` |\n",
        "| ~30 lines of code | ~5 lines of code |\n",
        "\n",
        "And more importantly: **Spark keeps data in memory** instead of writing to disk, making it much faster for complex pipelines!\n",
        "\n",
        "Now let's explore Spark's DataFrame API, which is even easier to use..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0495a61",
      "metadata": {
        "id": "e0495a61"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 4: Quick Pandas Refresher\n",
        "\n",
        "Before diving deeper into Spark DataFrames, let's create some sample data with Pandas â€” a tool you're probably already familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722aea40",
      "metadata": {
        "id": "722aea40"
      },
      "outputs": [],
      "source": [
        "# Create a simple sample dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "data = {\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
        "    'age': [25, 30, 35, 28, 22],\n",
        "    'city': ['Stockholm', 'Gothenburg', 'MalmÃ¶', 'Uppsala', 'Lund'],\n",
        "    'salary': [45000, 52000, 48000, 55000, 42000]\n",
        "}\n",
        "\n",
        "df_pandas = pd.DataFrame(data)\n",
        "print(\"Our sample dataset:\")\n",
        "df_pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d12771",
      "metadata": {
        "id": "d0d12771"
      },
      "outputs": [],
      "source": [
        "# Basic Pandas operations\n",
        "print(\"Shape:\", df_pandas.shape)\n",
        "print(\"\\nDescribe:\")\n",
        "df_pandas.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76bb867a",
      "metadata": {
        "id": "76bb867a"
      },
      "outputs": [],
      "source": [
        "# Filtering in Pandas\n",
        "high_earners = df_pandas[df_pandas['salary'] > 45000]\n",
        "print(\"People earning more than 45,000:\")\n",
        "high_earners"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d2b6b91",
      "metadata": {
        "id": "0d2b6b91"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 5: Your First Spark DataFrame\n",
        "\n",
        "Now let's do the same thing with Spark! We'll convert our Pandas DataFrame to a Spark DataFrame and see how similar (and different) the operations are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e24de1",
      "metadata": {
        "id": "20e24de1"
      },
      "outputs": [],
      "source": [
        "# Convert Pandas DataFrame to Spark DataFrame\n",
        "df_spark = spark.createDataFrame(df_pandas)\n",
        "\n",
        "# View the schema (data types)\n",
        "print(\"Spark DataFrame Schema:\")\n",
        "df_spark.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2541c28d",
      "metadata": {
        "id": "2541c28d"
      },
      "outputs": [],
      "source": [
        "# Show the data (like .head() in Pandas, but for Spark)\n",
        "df_spark.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906ca9e0",
      "metadata": {
        "id": "906ca9e0"
      },
      "outputs": [],
      "source": [
        "# Select specific columns\n",
        "df_spark.select('name', 'salary').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f0fe13",
      "metadata": {
        "id": "15f0fe13"
      },
      "outputs": [],
      "source": [
        "# Filter rows (same as Pandas, slightly different syntax)\n",
        "high_earners_spark = df_spark.filter(df_spark['salary'] > 45000)\n",
        "print(\"People earning more than 45,000 (Spark):\")\n",
        "high_earners_spark.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebdce81b",
      "metadata": {
        "id": "ebdce81b"
      },
      "source": [
        "### Pandas vs Spark: Quick Comparison\n",
        "\n",
        "| Operation | Pandas | PySpark |\n",
        "|-----------|--------|---------|\n",
        "| View data | `df.head()` | `df.show()` |\n",
        "| Select columns | `df[['col1', 'col2']]` | `df.select('col1', 'col2')` |\n",
        "| Filter rows | `df[df['col'] > 5]` | `df.filter(df['col'] > 5)` |\n",
        "| Data types | `df.dtypes` | `df.printSchema()` |\n",
        "| Row count | `len(df)` | `df.count()` |\n",
        "| Add column | `df['new'] = ...` | `df.withColumn('new', ...)` |\n",
        "\n",
        "The syntax is intentionally similar â€” making it easier to transition from Pandas to Spark!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f72c10",
      "metadata": {
        "id": "86f72c10"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 6: Why Spark? A Scaling Demo\n",
        "\n",
        "With 5 rows, Pandas is actually faster than Spark (overhead matters!). Let's see what happens when we scale up to 100,000 rows.\n",
        "\n",
        "### The Key Insight\n",
        "Spark's power isn't for small data â€” it's for data that doesn't fit in memory, or when you need to distribute work across a cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa7d65f",
      "metadata": {
        "id": "8fa7d65f"
      },
      "outputs": [],
      "source": [
        "# Generate a larger dataset\n",
        "np.random.seed(42)\n",
        "n_rows = 100_000\n",
        "\n",
        "cities = ['Stockholm', 'Gothenburg', 'MalmÃ¶', 'Uppsala', 'Lund',\n",
        "          'VÃ¤sterÃ¥s', 'Ã–rebro', 'LinkÃ¶ping', 'Helsingborg', 'JÃ¶nkÃ¶ping']\n",
        "\n",
        "large_data = {\n",
        "    'id': range(n_rows),\n",
        "    'age': np.random.randint(18, 70, n_rows),\n",
        "    'city': np.random.choice(cities, n_rows),\n",
        "    'salary': np.random.randint(30000, 100000, n_rows),\n",
        "    'years_experience': np.random.randint(0, 40, n_rows)\n",
        "}\n",
        "\n",
        "df_large_pandas = pd.DataFrame(large_data)\n",
        "print(f\"Created dataset with {len(df_large_pandas):,} rows\")\n",
        "print(f\"Memory usage: {df_large_pandas.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "df_large_pandas.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7948691e",
      "metadata": {
        "id": "7948691e"
      },
      "outputs": [],
      "source": [
        "# Convert to Spark DataFrame\n",
        "df_large_spark = spark.createDataFrame(df_large_pandas)\n",
        "\n",
        "# Check how Spark partitions the data\n",
        "print(f\"Number of partitions: {df_large_spark.rdd.getNumPartitions()}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df_large_spark.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4ef064",
      "metadata": {
        "id": "8d4ef064"
      },
      "outputs": [],
      "source": [
        "# Compare: Pandas aggregation\n",
        "start = time.time()\n",
        "result_pandas = df_large_pandas.groupby('city')['salary'].agg(['mean', 'max', 'min', 'count'])\n",
        "pandas_time = time.time() - start\n",
        "\n",
        "print(f\"Pandas aggregation took: {pandas_time*1000:.2f} ms\")\n",
        "result_pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e3f8e8",
      "metadata": {
        "id": "45e3f8e8"
      },
      "outputs": [],
      "source": [
        "# Compare: Spark aggregation\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "start = time.time()\n",
        "result_spark = df_large_spark.groupBy('city').agg(\n",
        "    F.avg('salary').alias('mean'),\n",
        "    F.max('salary').alias('max'),\n",
        "    F.min('salary').alias('min'),\n",
        "    F.count('salary').alias('count')\n",
        ").collect()  # .collect() triggers actual computation\n",
        "spark_time = time.time() - start\n",
        "\n",
        "print(f\"Spark aggregation took: {spark_time*1000:.2f} ms\")\n",
        "print(\"\\n(Spark is slower here because of overhead â€” but scales to billions of rows!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335ef4f6",
      "metadata": {
        "id": "335ef4f6"
      },
      "outputs": [],
      "source": [
        "# Side-by-side comparison: 100,000 rows\n",
        "print(\"=\" * 50)\n",
        "print(f\"ðŸ“Š SPEED COMPARISON: 100,000 rows\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Pandas:  {pandas_time*1000:>8.2f} ms\")\n",
        "print(f\"  Spark:   {spark_time*1000:>8.2f} ms\")\n",
        "print(f\"  Winner:  {'Pandas' if pandas_time < spark_time else 'Spark'} ({'%.1fx' % (spark_time/pandas_time) if pandas_time < spark_time else '%.1fx' % (pandas_time/spark_time)} faster)\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nðŸ’¡ Lesson: At this scale, Pandas wins! Spark has startup\")\n",
        "print(\"   overhead that only pays off with much larger data.\")\n",
        "print(\"\\n   Spark shines when:\")\n",
        "print(\"   â€¢ Data doesn't fit in memory (100+ GB)\")\n",
        "print(\"   â€¢ You have a cluster with many nodes\")\n",
        "print(\"   â€¢ You need fault tolerance for long-running jobs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0e0762c",
      "metadata": {
        "id": "e0e0762c"
      },
      "source": [
        "### â³ Optional: Big Data Showdown (10 million rows)\n",
        "\n",
        "The cell below creates a **10 million row** dataset â€” 100x larger than before. This will take **1-2 minutes** to run, but you'll finally see Spark's overhead become worthwhile.\n",
        "\n",
        "> **Skip this if short on time** â€” the concept is what matters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed94276d",
      "metadata": {
        "id": "ed94276d"
      },
      "outputs": [],
      "source": [
        "# âš ï¸ This cell takes 1-2 minutes to run!\n",
        "# 10 million rows â€” now we're talking big data\n",
        "\n",
        "n_rows_big = 10_000_000  # 10 million rows\n",
        "\n",
        "print(f\"Generating {n_rows_big:,} rows... (this takes a moment)\")\n",
        "start_gen = time.time()\n",
        "\n",
        "big_data = {\n",
        "    'id': range(n_rows_big),\n",
        "    'age': np.random.randint(18, 70, n_rows_big),\n",
        "    'city': np.random.choice(cities, n_rows_big),\n",
        "    'salary': np.random.randint(30000, 100000, n_rows_big),\n",
        "    'years_experience': np.random.randint(0, 40, n_rows_big)\n",
        "}\n",
        "\n",
        "df_big_pandas = pd.DataFrame(big_data)\n",
        "print(f\"âœ“ Created Pandas DataFrame in {time.time() - start_gen:.1f}s\")\n",
        "print(f\"  Memory usage: {df_big_pandas.memory_usage(deep=True).sum() / 1024**2:.0f} MB\")\n",
        "\n",
        "# Convert to Spark\n",
        "start_convert = time.time()\n",
        "df_big_spark = spark.createDataFrame(df_big_pandas)\n",
        "df_big_spark.cache()  # Cache in memory for fair comparison\n",
        "df_big_spark.count()  # Force materialization\n",
        "convert_time = time.time() - start_convert\n",
        "print(f\"âœ“ Created Spark DataFrame in {convert_time:.1f}s\")\n",
        "\n",
        "# Pandas timing\n",
        "start = time.time()\n",
        "result_pandas_big = df_big_pandas.groupby('city')['salary'].agg(['mean', 'max', 'min', 'count'])\n",
        "pandas_time_big = time.time() - start\n",
        "\n",
        "# Spark timing\n",
        "start = time.time()\n",
        "result_spark_big = df_big_spark.groupBy('city').agg(\n",
        "    F.avg('salary').alias('mean'),\n",
        "    F.max('salary').alias('max'),\n",
        "    F.min('salary').alias('min'),\n",
        "    F.count('salary').alias('count')\n",
        ").collect()\n",
        "spark_time_big = time.time() - start\n",
        "\n",
        "# Results\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(f\"ðŸ“Š SPEED COMPARISON: 10,000,000 rows\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Pandas:  {pandas_time_big*1000:>8.0f} ms\")\n",
        "print(f\"  Spark:   {spark_time_big*1000:>8.0f} ms\")\n",
        "winner = 'Pandas' if pandas_time_big < spark_time_big else 'Spark'\n",
        "print(f\"  Winner:  {winner}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if pandas_time_big <= spark_time_big:\n",
        "    print(\"\\nðŸ¤” Wait â€” Pandas still wins? Yes! Here's the lesson:\")\n",
        "    print(\"   â€¢ On a SINGLE machine, Pandas is hard to beat\")\n",
        "    print(\"   â€¢ Spark's overhead (coordination, serialization) hurts locally\")\n",
        "    print(\"   â€¢ Spark shines when data is ALREADY distributed across a cluster\")\n",
        "    print(\"   â€¢ Or when data is too big for RAM (try 100GB â€” Pandas crashes!)\")\n",
        "else:\n",
        "    print(\"\\nðŸ”¥ Spark is catching up! With more data or a real cluster,\")\n",
        "    print(\"   Spark would pull ahead significantly.\")\n",
        "\n",
        "print(f\"\\nâš ï¸ Note: Converting Pandasâ†’Spark took {convert_time:.0f}s â€” in real pipelines,\")\n",
        "print(\"   you'd read directly into Spark from disk/cloud, not via Pandas.\")\n",
        "\n",
        "# Clean up to free memory\n",
        "df_big_spark.unpersist()\n",
        "del df_big_pandas, big_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9b52373",
      "metadata": {
        "id": "b9b52373"
      },
      "source": [
        "### ðŸ”¥ The Real Spark Advantage: Data Larger Than RAM\n",
        "\n",
        "The demos above converted Pandas â†’ Spark, which isn't realistic. In the real world, you'd **read data directly into Spark** from disk, cloud storage, or databases.\n",
        "\n",
        "The cell below generates **100 million rows directly in Spark** â€” no Pandas involved. This would require ~9 GB of RAM in Pandas, but Spark handles it by:\n",
        "- Generating data lazily (doesn't load everything into RAM at once)\n",
        "- Spilling to disk when memory is tight\n",
        "- Processing in partitions, not all at once\n",
        "\n",
        "> ðŸ’¡ This runs in **1-30 seconds** depending on your hardware â€” much faster than you'd expect for 100M rows! That's the power of lazy evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c1bffc3",
      "metadata": {
        "id": "9c1bffc3"
      },
      "outputs": [],
      "source": [
        "# Generate 100 million rows DIRECTLY in Spark â€” no Pandas, no RAM limit!\n",
        "from pyspark.sql.functions import rand, floor, array, lit, element_at, when\n",
        "\n",
        "n_rows_massive = 100_000_000  # 100 million rows\n",
        "\n",
        "print(f\"ðŸš€ Generating {n_rows_massive:,} rows directly in Spark...\")\n",
        "print(\"   (This would require ~9 GB in Pandas â€” Spark handles it lazily)\\n\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Create cities array for random selection\n",
        "cities_array = array([lit(c) for c in cities])\n",
        "\n",
        "# Generate data directly in Spark â€” never touches Pandas!\n",
        "df_massive = (\n",
        "    spark.range(n_rows_massive)\n",
        "    .withColumn(\"age\", (floor(rand() * 52) + 18).cast(\"int\"))\n",
        "    .withColumn(\"city\", element_at(cities_array, (floor(rand() * 10) + 1).cast(\"int\")))\n",
        "    .withColumn(\"years_experience\", (floor(rand() * 40)).cast(\"int\"))\n",
        ")\n",
        "\n",
        "# Add salary with meaningful differences by city (reflects Swedish salary patterns)\n",
        "df_massive = df_massive.withColumn(\n",
        "    \"salary\",\n",
        "    when(df_massive[\"city\"] == \"Stockholm\", (floor(rand() * 40000) + 55000).cast(\"int\"))  # 55-95k\n",
        "    .when(df_massive[\"city\"] == \"Gothenburg\", (floor(rand() * 35000) + 50000).cast(\"int\"))  # 50-85k\n",
        "    .when(df_massive[\"city\"] == \"MalmÃ¶\", (floor(rand() * 30000) + 48000).cast(\"int\"))  # 48-78k\n",
        "    .when(df_massive[\"city\"] == \"Uppsala\", (floor(rand() * 30000) + 47000).cast(\"int\"))  # 47-77k\n",
        "    .when(df_massive[\"city\"] == \"LinkÃ¶ping\", (floor(rand() * 28000) + 45000).cast(\"int\"))  # 45-73k\n",
        "    .when(df_massive[\"city\"] == \"VÃ¤sterÃ¥s\", (floor(rand() * 25000) + 43000).cast(\"int\"))  # 43-68k\n",
        "    .when(df_massive[\"city\"] == \"Ã–rebro\", (floor(rand() * 25000) + 42000).cast(\"int\"))  # 42-67k\n",
        "    .when(df_massive[\"city\"] == \"Helsingborg\", (floor(rand() * 25000) + 41000).cast(\"int\"))  # 41-66k\n",
        "    .when(df_massive[\"city\"] == \"JÃ¶nkÃ¶ping\", (floor(rand() * 23000) + 40000).cast(\"int\"))  # 40-63k\n",
        "    .otherwise((floor(rand() * 22000) + 38000).cast(\"int\"))  # Lund: 38-60k\n",
        ")\n",
        "\n",
        "# Force execution with an aggregation\n",
        "result = df_massive.groupBy(\"city\").agg(\n",
        "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
        "    F.count(\"*\").alias(\"count\")\n",
        ").collect()\n",
        "\n",
        "total_time = time.time() - start\n",
        "\n",
        "print(f\"âœ“ Processed {n_rows_massive:,} rows in {total_time:.1f}s\")\n",
        "print(f\"âœ“ Partitions used: {df_massive.rdd.getNumPartitions()}\")\n",
        "print(f\"\\nResults (note the salary differences by city!):\")\n",
        "for row in sorted(result, key=lambda x: -x[\"avg_salary\"]):\n",
        "    print(f\"   {row['city']:12} avg salary: {row['avg_salary']:,.0f} SEK ({row['count']:,} rows)\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Key insight: Spark never loaded 100M rows into RAM at once!\")\n",
        "print(f\"   It processed them in partitions, streaming through memory.\")\n",
        "print(f\"   This same code works for 1B or 10B rows on a cluster.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469d26fb",
      "metadata": {
        "id": "469d26fb"
      },
      "source": [
        "### ðŸ” Check the Spark UI Now!\n",
        "\n",
        "Go back to the Spark UI (link from Section 2) and click on **Jobs**. You should see:\n",
        "\n",
        "1. Jobs that were created when we ran the aggregation\n",
        "2. Click on a job to see its **stages**\n",
        "3. Click on a stage to see the **tasks** that ran on each partition\n",
        "\n",
        "This visualization becomes essential when optimizing real big data jobs!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21499b08",
      "metadata": {
        "id": "21499b08"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 7: Understanding Partitions\n",
        "\n",
        "**Partitions** are how Spark divides data for parallel processing. Understanding partitions is crucial for performance.\n",
        "\n",
        "Think of it like dividing a pizza â€” more slices means more people can eat at once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec5c44f",
      "metadata": {
        "id": "2ec5c44f"
      },
      "outputs": [],
      "source": [
        "# See how data is distributed across partitions\n",
        "partition_counts = df_large_spark.rdd.mapPartitions(\n",
        "    lambda x: [sum(1 for _ in x)]\n",
        ").collect()\n",
        "\n",
        "print(f\"Data distribution across {len(partition_counts)} partitions:\")\n",
        "for i, count in enumerate(partition_counts):\n",
        "    bar = 'â–ˆ' * (count // 500)  # Visual representation\n",
        "    print(f\"  Partition {i}: {count:,} rows {bar}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d1a8bc",
      "metadata": {
        "id": "15d1a8bc"
      },
      "outputs": [],
      "source": [
        "# Repartition to change the number of partitions\n",
        "df_repartitioned = df_large_spark.repartition(4)\n",
        "print(f\"Original partitions: {df_large_spark.rdd.getNumPartitions()}\")\n",
        "print(f\"After repartition(4): {df_repartitioned.rdd.getNumPartitions()}\")\n",
        "\n",
        "# See new distribution\n",
        "new_counts = df_repartitioned.rdd.mapPartitions(\n",
        "    lambda x: [sum(1 for _ in x)]\n",
        ").collect()\n",
        "\n",
        "print(\"\\nNew distribution:\")\n",
        "for i, count in enumerate(new_counts):\n",
        "    bar = 'â–ˆ' * (count // 1000)\n",
        "    print(f\"  Partition {i}: {count:,} rows {bar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7273cd7",
      "metadata": {
        "id": "d7273cd7"
      },
      "source": [
        "### Why Partitions Matter\n",
        "\n",
        "- **Too few partitions**: Not enough parallelism, some cores sit idle\n",
        "- **Too many partitions**: Overhead of managing many small tasks\n",
        "- **Uneven partitions**: Some tasks finish early, others take forever (\"data skew\")\n",
        "\n",
        "Rule of thumb: 2-4 partitions per CPU core for local mode."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d1f8c6",
      "metadata": {
        "id": "31d1f8c6"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ‹ï¸ Exercise B: Partition Experiment\n",
        "\n",
        "Partitions have a real impact on performance. In this exercise, you'll measure it!\n",
        "\n",
        "### Your Task\n",
        "\n",
        "1. Create a DataFrame with **1 million rows** (modify the code below)\n",
        "2. Time an aggregation with the **default number of partitions**\n",
        "3. Repartition to **2 partitions** and time the same aggregation\n",
        "4. Repartition to **100 partitions** and time again\n",
        "5. Discuss with your group: What's the optimal number? Why?\n",
        "\n",
        "### Guiding Questions\n",
        "- What happens when you have too few partitions?\n",
        "- What happens when you have too many?\n",
        "- How does this relate to the number of CPU cores on your machine?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d567b063",
      "metadata": {
        "id": "d567b063"
      },
      "outputs": [],
      "source": [
        "# Exercise B: Partition Experiment\n",
        "# Step 1: Create a larger dataset (1 million rows)\n",
        "\n",
        "n_rows_experiment = 1_000_000  # 1 million rows\n",
        "\n",
        "experiment_data = {\n",
        "    'id': range(n_rows_experiment),\n",
        "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows_experiment),\n",
        "    'value': np.random.randint(1, 1000, n_rows_experiment),\n",
        "}\n",
        "\n",
        "df_experiment_pandas = pd.DataFrame(experiment_data)\n",
        "df_experiment = spark.createDataFrame(df_experiment_pandas)\n",
        "\n",
        "print(f\"Created dataset with {n_rows_experiment:,} rows\")\n",
        "print(f\"Default partitions: {df_experiment.rdd.getNumPartitions()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a1f22c",
      "metadata": {
        "id": "13a1f22c"
      },
      "outputs": [],
      "source": [
        "# Step 2: Define the aggregation we'll test\n",
        "def time_aggregation(df, label):\n",
        "    \"\"\"Time a groupBy aggregation and return the duration.\"\"\"\n",
        "    start = time.time()\n",
        "    result = df.groupBy('category').agg(\n",
        "        F.avg('value').alias('avg'),\n",
        "        F.sum('value').alias('sum'),\n",
        "        F.count('value').alias('count')\n",
        "    ).collect()\n",
        "    duration = time.time() - start\n",
        "    print(f\"{label}: {duration*1000:.0f} ms ({df.rdd.getNumPartitions()} partitions)\")\n",
        "    return duration\n",
        "\n",
        "# Test with default partitions\n",
        "time_default = time_aggregation(df_experiment, \"Default\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cfa5060",
      "metadata": {
        "id": "6cfa5060"
      },
      "outputs": [],
      "source": [
        "# Step 3: Test with 2 partitions (very few)\n",
        "df_2_partitions = df_experiment.coalesce(2)\n",
        "time_2 = time_aggregation(df_2_partitions, \"2 partitions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b91d1afb",
      "metadata": {
        "id": "b91d1afb"
      },
      "outputs": [],
      "source": [
        "# Step 4: Test with 100 partitions (many)\n",
        "df_100_partitions = df_experiment.repartition(100)\n",
        "time_100 = time_aggregation(df_100_partitions, \"100 partitions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb16ddf",
      "metadata": {
        "id": "beb16ddf"
      },
      "outputs": [],
      "source": [
        "# Step 5: Compare results and discuss\n",
        "print(\"\\nðŸ“Š Performance Summary:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Default ({df_experiment.rdd.getNumPartitions()} partitions): {time_default*1000:.0f} ms\")\n",
        "print(f\"2 partitions:   {time_2*1000:.0f} ms\")\n",
        "print(f\"100 partitions: {time_100*1000:.0f} ms\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"\\nðŸ¤” Discussion questions for your group:\")\n",
        "print(\"1. Which configuration was fastest? Why?\")\n",
        "print(\"2. How many CPU cores does your machine have? (Check above)\")\n",
        "print(\"3. What would happen with 1000 partitions?\")\n",
        "print(\"4. When might you want MORE partitions than cores?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d473ca",
      "metadata": {
        "id": "91d473ca"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 8: Lazy Evaluation Preview\n",
        "\n",
        "One of Spark's superpowers is **lazy evaluation**. Transformations aren't executed until you need the result. This allows Spark to optimize the entire pipeline!\n",
        "\n",
        "We'll explore this more deeply in Lab 1, but here's a preview:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f037fc45",
      "metadata": {
        "id": "f037fc45"
      },
      "outputs": [],
      "source": [
        "# This is INSTANT - nothing is computed yet!\n",
        "start = time.time()\n",
        "\n",
        "# Chain multiple transformations\n",
        "result = df_large_spark \\\n",
        "    .filter(df_large_spark['age'] > 30) \\\n",
        "    .filter(df_large_spark['salary'] > 50000) \\\n",
        "    .select('city', 'salary') \\\n",
        "    .groupBy('city') \\\n",
        "    .agg(F.avg('salary').alias('avg_salary'))\n",
        "\n",
        "planning_time = time.time() - start\n",
        "print(f\"âš¡ Planning took: {planning_time*1000:.2f} ms\")\n",
        "print(\"\\nNothing has been computed yet! Spark just built a plan.\")\n",
        "print(f\"Type of result: {type(result)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d41960c",
      "metadata": {
        "id": "6d41960c"
      },
      "outputs": [],
      "source": [
        "# See the query plan\n",
        "print(\"Query plan (what Spark will do):\")\n",
        "result.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eee165e",
      "metadata": {
        "id": "7eee165e"
      },
      "outputs": [],
      "source": [
        "# NOW it executes - when we ask for results\n",
        "start = time.time()\n",
        "actual_result = result.collect()  # This triggers computation!\n",
        "execution_time = time.time() - start\n",
        "\n",
        "print(f\"â±ï¸ Execution took: {execution_time*1000:.2f} ms\")\n",
        "print(\"\\nResults:\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41b2d44f",
      "metadata": {
        "id": "41b2d44f"
      },
      "source": [
        "### Actions vs Transformations\n",
        "\n",
        "| Transformations (Lazy) | Actions (Trigger Execution) |\n",
        "|----------------------|---------------------------|\n",
        "| `filter()` | `show()` |\n",
        "| `select()` | `count()` |\n",
        "| `groupBy()` | `collect()` |\n",
        "| `join()` | `write()` |\n",
        "| `withColumn()` | `take(n)` |\n",
        "\n",
        "Transformations build a plan. Actions execute it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9996f05",
      "metadata": {
        "id": "f9996f05"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 9: Exercises & Exploration\n",
        "\n",
        "The final section contains two exercises and a free exploration sandbox. Work through these with your group!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da298022",
      "metadata": {
        "id": "da298022"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ‹ï¸ Exercise C: Spark UI Scavenger Hunt\n",
        "\n",
        "The Spark UI is your best friend for understanding and debugging Spark jobs. Let's explore it!\n",
        "\n",
        "> âš ï¸ **Colab Users:** Skip this exercise â€” the Spark UI isn't accessible in Colab. Instead, use `explain()` to examine query plans (see alternative below).\n",
        "\n",
        "### Your Mission\n",
        "\n",
        "Run the aggregation cell below, then open the Spark UI and find the answers to these questions:\n",
        "\n",
        "1. **Jobs Tab:** How many jobs were created?\n",
        "2. **Stages:** Click on a job â€” how many stages does it have?\n",
        "3. **Tasks:** Click on a stage â€” how many tasks ran in parallel?\n",
        "4. **Timeline:** Look at the Event Timeline â€” did tasks run concurrently?\n",
        "5. **Shuffle:** Find the shuffle read/write metrics â€” how much data was shuffled?\n",
        "\n",
        "### For Colab Users (Alternative)\n",
        "Use `.explain(True)` to see the physical plan instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65737565",
      "metadata": {
        "id": "65737565"
      },
      "outputs": [],
      "source": [
        "# Run this aggregation, then explore the Spark UI\n",
        "scavenger_result = df_large_spark.groupBy('city').agg(\n",
        "    F.avg('salary').alias('avg_salary'),\n",
        "    F.max('years_experience').alias('max_experience'),\n",
        "    F.count('*').alias('employee_count')\n",
        ").orderBy(F.desc('avg_salary'))\n",
        "\n",
        "# Trigger execution\n",
        "scavenger_result.show()\n",
        "\n",
        "# Print the Spark UI URL again for convenience\n",
        "print(f\"\\nðŸŒ Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
        "print(\"   Go to the Jobs tab and find the job that just ran!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98b1e1e",
      "metadata": {
        "id": "f98b1e1e"
      },
      "outputs": [],
      "source": [
        "# Alternative for Colab users: Examine the query plan\n",
        "print(\"Query Plan (what Spark will execute):\")\n",
        "print(\"=\" * 50)\n",
        "scavenger_result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195278da",
      "metadata": {
        "id": "195278da"
      },
      "source": [
        "### ðŸ“– How to Read the Query Plan\n",
        "\n",
        "The output above shows **4 different views** of the same query, from abstract to concrete:\n",
        "\n",
        "**1. Parsed Logical Plan** â€” What you wrote, parsed into a tree structure\n",
        "- Just translates your code, no optimizations yet\n",
        "\n",
        "**2. Analyzed Logical Plan** â€” Same as above, but with resolved column types\n",
        "- Now Spark knows `avg_salary` is a `double`, `employee_count` is a `bigint`, etc.\n",
        "\n",
        "**3. Optimized Logical Plan** â€” Spark's optimizer has improved your query!\n",
        "- Notice the `Project` step â€” Spark realized it only needs 3 columns (`city`, `salary`, `years_experience`), so it drops `id` and `age` early to save memory\n",
        "\n",
        "**4. Physical Plan** â€” The actual execution strategy (this is what runs!)\n",
        "\n",
        "### ðŸ” Reading the Physical Plan (Bottom to Top!)\n",
        "\n",
        "Read from the **bottom up** â€” that's the order of execution:\n",
        "\n",
        "```\n",
        "Scan ExistingRDD           â† 1. Read the data\n",
        "   â†“\n",
        "Project [city, salary, years_experience]  â† 2. Keep only needed columns\n",
        "   â†“\n",
        "HashAggregate (partial)    â† 3. Partial aggregation BEFORE shuffle (optimization!)\n",
        "   â†“\n",
        "Exchange hashpartitioning  â† 4. SHUFFLE! Send data to reducers by city\n",
        "   â†“\n",
        "HashAggregate (final)      â† 5. Final aggregation after shuffle\n",
        "   â†“\n",
        "Exchange rangepartitioning â† 6. Another shuffle for sorting\n",
        "   â†“\n",
        "Sort                       â† 7. Sort by avg_salary descending\n",
        "```\n",
        "\n",
        "### ðŸŽ¯ Key Insights\n",
        "\n",
        "- **Two shuffles!** (`Exchange` = shuffle). One for grouping by city, one for sorting.\n",
        "- **Partial aggregation**: Spark computes partial averages/counts *before* shuffling (see `partial_avg`, `partial_count`). This reduces network traffic â€” smart!\n",
        "- **AdaptiveSparkPlan**: Spark can adjust the plan at runtime based on actual data sizes.\n",
        "- **200 partitions**: The default shuffle partition count (you can tune this with `spark.sql.shuffle.partitions`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d802648",
      "metadata": {
        "id": "6d802648"
      },
      "source": [
        "### Scavenger Hunt Answers (Fill in with your group)\n",
        "\n",
        "| Question | Your Answer |\n",
        "|----------|-------------|\n",
        "| How many jobs? | ___ |\n",
        "| Stages per job? | ___ |\n",
        "| Tasks per stage? | ___ |\n",
        "| Concurrent execution? | Yes / No |\n",
        "| Shuffle data size? | ___ MB |\n",
        "\n",
        "**Discussion:** Why are there multiple stages? (Hint: What operation requires data movement?)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e7318a",
      "metadata": {
        "id": "e2e7318a"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ‹ï¸ Exercise D: Distributed Thinking\n",
        "\n",
        "This is a **group discussion exercise** â€” no code required! Work through these scenarios with your group.\n",
        "\n",
        "### Scenario 1: Web Log Analysis\n",
        "\n",
        "You work at a streaming company and need to analyze **500 GB of web server logs** to find:\n",
        "- The most popular pages\n",
        "- Peak usage hours\n",
        "- Users with unusual behavior\n",
        "\n",
        "**Discussion Questions:**\n",
        "1. Would this fit in Pandas on your laptop? (Hint: How much RAM do you have?)\n",
        "2. If you have a cluster with 10 nodes, how many partitions would you choose?\n",
        "3. What if one log file is 400 GB and the others are 10 GB each? What problem might occur?\n",
        "\n",
        "### Scenario 2: Data Pipeline Failure\n",
        "\n",
        "Your nightly Spark job that processes customer orders has been running for 8 hours (usually takes 1 hour). You open the Spark UI and see:\n",
        "- 199 out of 200 tasks completed\n",
        "- 1 task has been running for 7.5 hours\n",
        "\n",
        "**Discussion Questions:**\n",
        "1. What is this symptom called?\n",
        "2. What might cause one task to take so long?\n",
        "3. How would you investigate and fix this?\n",
        "\n",
        "### Scenario 3: Real-time vs Batch\n",
        "\n",
        "Your team debates whether to process data in real-time or in nightly batches.\n",
        "\n",
        "**Discussion Questions:**\n",
        "1. What are the trade-offs between batch and streaming?\n",
        "2. Give an example where real-time processing is essential\n",
        "3. Give an example where batch processing is sufficient"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cadaa2ce",
      "metadata": {
        "id": "cadaa2ce"
      },
      "source": [
        "### ðŸ“ Group Notes: Record Your Answers\n",
        "\n",
        "Use this space to record your group's answers and insights:\n",
        "\n",
        "**Scenario 1 (Web Logs):**\n",
        "- Would it fit in Pandas?\n",
        "- Suggested partition count?\n",
        "- The 400 GB file problem is called...\n",
        "\n",
        "**Scenario 2 (Pipeline Failure):**\n",
        "- The symptom is called...\n",
        "- Possible causes...\n",
        "- Investigation steps...\n",
        "\n",
        "**Scenario 3 (Real-time vs Batch):**\n",
        "- Trade-offs...\n",
        "- Real-time example...\n",
        "- Batch example..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a097ca",
      "metadata": {
        "id": "79a097ca"
      },
      "source": [
        "### ðŸ§ª Exploration Sandbox\n",
        "\n",
        "Use these cells to experiment with Spark! Try different operations, break things, and learn by doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcc84f81",
      "metadata": {
        "id": "dcc84f81"
      },
      "outputs": [],
      "source": [
        "# Try: Summary statistics\n",
        "df_large_spark.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50979aad",
      "metadata": {
        "id": "50979aad"
      },
      "outputs": [],
      "source": [
        "# Try: Top 10 highest salaries\n",
        "df_large_spark.orderBy(df_large_spark['salary'].desc()).show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9e3dd3",
      "metadata": {
        "id": "4f9e3dd3"
      },
      "outputs": [],
      "source": [
        "# Try: Add a calculated column\n",
        "df_with_tax = df_large_spark.withColumn('estimated_tax', df_large_spark['salary'] * 0.3)\n",
        "df_with_tax.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22200d34",
      "metadata": {
        "id": "22200d34"
      },
      "outputs": [],
      "source": [
        "# Your experiments here...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43417fdf",
      "metadata": {
        "id": "43417fdf"
      },
      "outputs": [],
      "source": [
        "# More space for exploration...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456c6008",
      "metadata": {
        "id": "456c6008"
      },
      "source": [
        "---\n",
        "\n",
        "## Wrap-up\n",
        "\n",
        "### What You Learned Today\n",
        "\n",
        "1. **Environment setup** â€” How to verify and configure PySpark\n",
        "2. **MapReduce** â€” The foundational paradigm (Map â†’ Shuffle â†’ Reduce)\n",
        "3. **SparkSession** â€” The entry point to Spark functionality\n",
        "4. **MapReduce vs Spark** â€” Same word count in 5 lines instead of 30!\n",
        "5. **Pandas â†’ Spark** â€” Converting DataFrames and the similar syntax\n",
        "6. **Scaling** â€” Why Spark matters for large datasets\n",
        "7. **Partitions** â€” How Spark distributes data for parallel processing\n",
        "8. **Lazy evaluation** â€” Transformations vs Actions\n",
        "9. **Spark UI** â€” Monitoring what Spark is doing\n",
        "10. **Distributed thinking** â€” Reasoning about data at scale\n",
        "\n",
        "### Key Concepts to Remember\n",
        "\n",
        "- **MapReduce** is the foundational pattern: Map (transform) â†’ Shuffle (group) â†’ Reduce (combine)\n",
        "- Spark is designed for **distributed** computing â€” it shines with big data\n",
        "- Spark keeps data **in memory** instead of writing to disk like Hadoop MapReduce\n",
        "- **Lazy evaluation** lets Spark optimize your entire pipeline\n",
        "- **Partitions** determine parallelism â€” balance is key\n",
        "- **Data skew** (uneven partitions) is a common performance killer\n",
        "- The **Spark UI** is your best friend for debugging and optimization\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "- **Next week (Week 5):** Lab 1 will be published â€” your first graded assignment!\n",
        "- Lab 1 dives deeper into transformations, actions, and the execution model\n",
        "- You'll work with larger datasets and submit your completed notebook\n",
        "\n",
        "### Great work today! ðŸŽ‰\n",
        "\n",
        "You've taken your first steps into distributed computing. The concepts you learned today â€” MapReduce thinking, partitioning, lazy evaluation â€” are foundational to everything else in this course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4b0b63",
      "metadata": {
        "id": "de4b0b63"
      },
      "outputs": [],
      "source": [
        "# Clean up - stop the Spark session when you're done\n",
        "spark.stop()\n",
        "print(\"âœ“ Spark session stopped. Learning lab complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}